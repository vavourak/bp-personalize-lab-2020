{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Personalize boto3 Client\n",
    "\n",
    "All of the code that we are using in this lab is Python, but any langugage supported by SageMaker could be used.  In this initial piece of code we are loading in the library dependencies that we need for the rest of the lab:\n",
    "\n",
    "- **boto3** - standard Python SDK that wraps the AWS CLI\n",
    "- **json** - used to manipulate JSON structures used by our API calls\n",
    "- **numpy** and **pandas** - standard libraries used by Data Scientists\n",
    "- **time** - used for some time manipulation calls\n",
    "- **display** - to display images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "personalize = boto3.client(service_name='personalize')\n",
    "personalize_runtime = boto3.client(service_name='personalize-runtime')\n",
    "\n",
    "# If you have to rerun the notebook, increment this suffix to avoid conflicts\n",
    "suffix = '1'\n",
    "\n",
    "# Retrieve suffix used by background notebook\n",
    "%store -r bg_suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Bucket and Data Output Location\n",
    "\n",
    "For this demo, we'll choose to upload data to Amazon Personalize directly from an S3 bucket.  Hence, you need to create a new bucket - please name your bucket before running this Code cell, overriding what is shown in the code cell, and you need to ensure that the bucket name is globally unique; for this lab we recommend using your name or initials, followed by *-bp-personalize-lab-*, as that is likely to be unique.\n",
    "\n",
    "If the bucket already exists - such as if you execute this code cell a second time - then it will not create a new bucket, and will not make any changes to the exsting bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket: cjv-bp-personalize-lab-1\n"
     ]
    }
   ],
   "source": [
    "# Replace with the name of your S3 bucket\n",
    "bucket = \"{your-prefix}-bp-personalize-lab-\" + suffix\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "if boto3.resource('s3').Bucket(bucket).creation_date is None:\n",
    "    s3.create_bucket(ACL = \"private\", Bucket = bucket)\n",
    "    print(\"Creating bucket: {}\".format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Prepare, and Upload Training Data\n",
    "\n",
    "#### Download and Explore the Dataset\n",
    "\n",
    "In this step we download the gas station data set zip-file and extract it - it will go in the same location as the physical notebook *.ipynb* file.  We use the **pandas** library to read in the *gas_station_data_5k.csv* file, which contains all of the in-store beverage purchases; this file consists of a user ID (vehicle category), item ID (beverage), a timestamp.  The dataset has been trimmed to 5k entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-16 16:30:23--  https://bp-personalize-lab-2020.s3.amazonaws.com/gas_station_data_5k.zip\n",
      "Resolving bp-personalize-lab-2020.s3.amazonaws.com (bp-personalize-lab-2020.s3.amazonaws.com)... 52.216.107.180\n",
      "Connecting to bp-personalize-lab-2020.s3.amazonaws.com (bp-personalize-lab-2020.s3.amazonaws.com)|52.216.107.180|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘gas_station_data_5k.zip’ not modified on server. Omitting download.\n",
      "\n",
      "Archive:  gas_station_data_5k.zip\n",
      "  inflating: gas_station_data_5k.csv  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>874725485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>874728370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>879440475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>879440545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      USER_ID  ITEM_ID  TIMESTAMP\n",
       "0          10        8  874725485\n",
       "1           2       21  874728370\n",
       "...       ...      ...        ...\n",
       "4998       12        6  879440475\n",
       "4999       12        7  879440545\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'gas_station_data_5k'\n",
    "\n",
    "!wget -N https://bp-personalize-lab-2020.s3.amazonaws.com/gas_station_data_5k.zip\n",
    "!unzip -o gas_station_data_5k.zip\n",
    "\n",
    "data = pd.read_csv('./gas_station_data_5k.csv', sep=',', names=['USER_ID', 'ITEM_ID', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare and Upload Data\n",
    "\n",
    "We don't actually need all of the purchase data.  We would like to recommend non-alcoholic beverages, as we do not know the age of the customer or the time of day.  We would not want to promote those beverages to minors or on a Sunday.  Hence, we're going to use **pandas** to drop the entries related to alcoholic beverages - we're left with a subset of the original, which are safe to promote.\n",
    "\n",
    "Additionally, the purchases are quite old - they are from August 1997 to April 1998.  Some of the Amazon Personalize recipies react differently depending upon the age of the interactions - for instance, the _Similar Items_ recipe has several hyperparameters around how to handle 'decaying' interactions.  In order to make this lab easier, and not have to worry about these hyperparameters, we are shifting all of the purchase timestamps to be from August 2018 up until April 2019.\n",
    "\n",
    "We then write that out to a file named and upload it into our S3 bucket.  \n",
    "\n",
    "This is the minimum amount of data that Amazon Personalize needs to train a model on - you need just 1500 rows of user/item/timestamp interactions, but we still have many thousands of entries left from our original dataset.  This file is known in Amazon Personalize as an **Interactions** data file.  Other data files could be usable, such as ones that define additional metadata about the beverages and another that defines demographic data about the user (such as age, gender and location).  In this lab we do not have this available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"gas_station_data.csv\"\n",
    "\n",
    "data = data[data['ITEM_ID'] < 15]                  # keep only non-alcoholic beverages\n",
    "data = data[['USER_ID', 'ITEM_ID', 'TIMESTAMP']]   # select columns that match the columns in the schema below\n",
    "data['TIMESTAMP'] = data['TIMESTAMP'] + 660833618  # make purchases end 1st April 2019 rather than 23rd April 1998\n",
    "data.to_csv(filename, index=False)\n",
    "\n",
    "# Upload data to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(filename).upload_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "Amazon Personalize uses *Schemas* to tell it how to interpret your data files.  This step defines the schema for our Interations file, which consists solely of a `USER_ID`, `ITEM_ID` and `TIMESTAMP`.  Once defined we pass it into Personalize for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:471551772664:schema/bp-pers-schema-1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"f0cb9a53-7626-496a-a2f9-c86ec5d3088b\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:31:08 GMT\",\n",
      "      \"x-amzn-requestid\": \"f0cb9a53-7626-496a-a2f9-c86ec5d3088b\",\n",
      "      \"content-length\": \"82\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "schema_name = \"bp-pers-schema-\" + suffix\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "# Create the Schema in Personalize\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = schema_name,\n",
    "    schema = json.dumps(schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group\n",
    "\n",
    "Now that we have defined a schema, and we have our Interactions data file, we can import the data into Personalize.  But first we have to define a *Dataset Group*, which is essentially a collection of imported data files, trained models and campaigns - each Dataset Group can contain one, and only one, Interaction, Item Metadata and User Demographic file.  When you train a model Personalize will use **all** data files present within its Dataset Group.\n",
    "\n",
    "#### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:471551772664:dataset-group/bp-pers-dataset-group1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"99d5872b-63a9-4e39-8dbb-f6d478984a15\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:31:20 GMT\",\n",
      "      \"x-amzn-requestid\": \"99d5872b-63a9-4e39-8dbb-f6d478984a15\",\n",
      "      \"content-length\": \"101\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"bp-personalize-dataset-group\" + suffix\n",
    "\n",
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = dataset_name\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status\n",
    "\n",
    "A number of Personalize API calls do take time, hence the calls are asynchronous.  Before we can continue with the next stage we need to poll the status of the `create_dataset_group()` call from the previous code cell - once the Dataset Group is active then we can continue.  **NOTE: this step should not take more than 1-2 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # wait up to 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "We now have to create our dataset for the Interactions file.  This step does not actually import any data, rather it creates an internal structure for the data to be imported into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:471551772664:dataset/bp-pers-dataset-group1/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"02303a4a-4e53-4a82-aa8f-76cf0fe64c06\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:36:00 GMT\",\n",
      "      \"x-amzn-requestid\": \"02303a4a-4e53-4a82-aa8f-76cf0fe64c06\",\n",
      "      \"content-length\": \"103\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn,\n",
    "    name=\"bp-pers-dataset-\" + suffix\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare, Create, and Wait for Dataset Import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach policy to S3 bucket\n",
    "\n",
    "Whilst we have created an S3 bucket, and our Interactions data file is sat there waiting to be imported, we have a problem - you may have full access to the bucket via the AWS console or APIs, but the Amazon Personalize service does not.  Hence, you have to create an S3 bucket policy that explicitly grants the service access to the `GetObject` and `ListBucket` commands in S3.  This code step creates such a policy and attaches it to your S3 bucket.\n",
    "\n",
    "Note, any Personalize API calls that need to access you S3 bucket need to be done using an IAM role that gives it permission - this step simply allows the service to access the bucket if, and only if, roles with appropriate permissions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Personalize S3 Role ARN\n",
    "\n",
    "As part of the AWS Event Engine process we have defined an IAM role that gives Personalize the ability to access S3 buckets - as mentioned this is needed as well as the S3 bucket policy.  As the Event Engine creates the IAM role via CloudFormation it will always have an essentially random numeric suffix, so we cannot hard-code it into the lab.  This code cell looks for any any service role that has the name _PersonalizeRoleForLab_ in it and selects it as the ARN that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::471551772664:role/pers-iam-role-PersonalizeRoleForLab-CO81EV7IVHGP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleForLab\"\n",
    "role_list = iam.list_roles()\n",
    "\n",
    "for role in role_list['Roles']:\n",
    "    if role_name in (role['Arn']):\n",
    "        role_arn = (role['Arn'])\n",
    "        \n",
    "role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job\n",
    "\n",
    "This pulls together the information that we have on our Dataset, on our S3 bucket, on our Interactions file and a suitable role for Personalize, and then triggers the actual data import process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:471551772664:dataset-import-job/bp-personalize-import-job1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"2813d85e-382d-4468-b633-8b8103365e8a\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:36:14 GMT\",\n",
      "      \"x-amzn-requestid\": \"2813d85e-382d-4468-b633-8b8103365e8a\",\n",
      "      \"content-length\": \"114\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataSource = {\"dataLocation\": \"s3://{}/{}\".format(bucket, filename)}\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"bp-personalize-import-job\" + suffix,\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\"dataLocation\": \"s3://{}/{}\".format(bucket, filename)},\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job and Dataset Import Job Run to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of Interactions file import job, as until it is complete we cannot continue.  **Note: this can take anything between 12-25 minutes to complete.** We will only wait for 3 minutes, then use the same dataset imported in the background, which should have completed by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "\n",
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Recipe\n",
    "\n",
    "There are many different algorithm recipes available within Personalize, this is a list of all supported algorithms at the time of the workshop.  We are going to select the standard HRNN recipe, which only needs the Interactions file and not the Item metadata or User demographic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:::recipe/aws-hrnn\n"
     ]
    }
   ],
   "source": [
    "recipe_list = [\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn\",\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn-coldstart\",\n",
    "    \"arn:aws:personalize:::recipe/aws-hrnn-metadata\",\n",
    "    \"arn:aws:personalize:::recipe/aws-personalized-ranking\",\n",
    "    \"arn:aws:personalize:::recipe/aws-popularity-count\",\n",
    "    \"arn:aws:personalize:::recipe/aws-sims\"\n",
    "]\n",
    "\n",
    "recipe_arn = recipe_list[0]\n",
    "print(recipe_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Solution\n",
    "\n",
    "With our data imported we can now train our ML solution.  This consists of just a single API to Personalize, where we specify the Dataset to use.\n",
    "\n",
    "#### Create Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:471551772664:solution/bp-beverage-solution1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"9d277a2f-c771-42be-8f01-b6d92489d99b\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:41:47 GMT\",\n",
      "      \"x-amzn-requestid\": \"9d277a2f-c771-42be-8f01-b6d92489d99b\",\n",
      "      \"content-length\": \"91\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%store -r dataset_group_arn\n",
    "\n",
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"bp-beverage-solution\" + suffix,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:471551772664:solution/bp-beverage-solution1/ec3dda5f\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"a229c22f-7f97-496e-8128-21563929d613\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 16:41:52 GMT\",\n",
      "      \"x-amzn-requestid\": \"a229c22f-7f97-496e-8128-21563929d613\",\n",
      "      \"content-length\": \"107\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = solution_arn\n",
    ")\n",
    "\n",
    "solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of solution creation job, as until it is complete we cannot continue. **Note: this can take anything between 25-50 minutes to complete**. Again, we will wait for a few minutes only, then use the solution generated in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolutionVersion: CREATE PENDING\n",
      "SolutionVersion: CREATE IN_PROGRESS\n",
      "SolutionVersion: CREATE IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "#max_time = time.time() + 3*60*60 # 3 hours\n",
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_solution_version_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = solution_version_arn\n",
    "    )\n",
    "    status = describe_solution_version_response[\"solutionVersion\"][\"status\"]\n",
    "    print(\"SolutionVersion: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Additional Solutions in the Console\n",
    "\n",
    "Whilst you're waiting for this to complete, jump back into the original Lab Guidebook - there we will walk you through creating two additional solutions in parallel using the same dataset; one for Personalized Rankings and one for Item-to-Item Similarities (or SIMS), both of which can be used in the final application.  Once you've begun to create both additional solutions you can come back here and continue.\n",
    "\n",
    "#### Get Metrics of Solution\n",
    "\n",
    "Once the solution is built you can look up the various metrics that Personalize provides - this allows you to see how well a model has been trained.  If you are re-training models after the acquisition of new data then these metrics can tell you if the models are training equally as well as before, better than before or worse than before, giving you the information that you need in order to decide whether or not to push a new model into Production.  You can also compare results across multiple different algorithm recipes, helping you choose the best performing one for you particular dataset.\n",
    "\n",
    "You can find details on each of the metrics in our [documentation](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:471551772664:solution/bp-beverage-solutionbackground1/b5bb675f\",\n",
      "  \"metrics\": {\n",
      "    \"coverage\": 1.0,\n",
      "    \"mean_reciprocal_rank_at_25\": 0.9,\n",
      "    \"normalized_discounted_cumulative_gain_at_10\": 0.7594,\n",
      "    \"normalized_discounted_cumulative_gain_at_25\": 0.8657,\n",
      "    \"normalized_discounted_cumulative_gain_at_5\": 0.7084,\n",
      "    \"precision_at_10\": 0.62,\n",
      "    \"precision_at_25\": 0.336,\n",
      "    \"precision_at_5\": 0.68\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"740daa50-c420-4bb8-acaa-fe81779d6c77\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 19:19:05 GMT\",\n",
      "      \"x-amzn-requestid\": \"740daa50-c420-4bb8-acaa-fe81779d6c77\",\n",
      "      \"content-length\": \"404\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%store -r solution_version_arn\n",
    "\n",
    "get_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(get_solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Campaign\n",
    "\n",
    "A trained model is exactly that - just a model.  In order to use it you need to create an API endpoint, and you do this by creating a *Campaign*.  A Campaign simply provides the endpoint for a specific version of your model, and as such you are able to host endpoint for multiple versions of your models simultaneously, allowing you to do things like A/B testing of new models.\n",
    "\n",
    "At the campaign level we specify the the minimum deployed size of the inference engine in terms of transactions per second - whilst this engine can scale up and down dynamically it will never scale below this level, but please note that pricing for Personalize is heavily based around the number of TPS currently deployed.\n",
    "\n",
    "#### Create campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"campaignArn\": \"arn:aws:personalize:us-east-1:471551772664:campaign/bp-beverage-campaign1\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e58fe2b1-b152-4a53-879e-c23fff6da9fd\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 16 Mar 2020 19:19:08 GMT\",\n",
      "      \"x-amzn-requestid\": \"e58fe2b1-b152-4a53-879e-c23fff6da9fd\",\n",
      "      \"content-length\": \"91\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"bp-beverage-campaign\" + suffix,\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    minProvisionedTPS = 1\n",
    ")\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to Have ACTIVE Status\n",
    "\n",
    "We now poll the status of Campaign creation job, as until it is complete we cannot continue.  **Note: this can take anything between 3-15 minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign: CREATE PENDING\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    print(\"Campaign: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations\n",
    "\n",
    "Finally, we have a deployed Campaign end-point, which hosts a specific version of our trained model - we are now able to make recommendation inference requests against it.  However, the Personalize recommendation calls just return itemID values - it returns no context around the title of the movie,  Hence, we use our pre-loaded version of the *beverages.csv* file that contains the beverage types - we load in the file via **pandas** library calls and pick out a random drink ID (ITEM_ID) and vehicle ID (USER_ID) from our training data.  This info is displayed, along with the name of the beverage.\n",
    "\n",
    "#### Select a User and an Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:               SUV, New\n",
      "Previous Purchase:  Sports Drink\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('./vehicles.csv', sep=',', usecols=[0,1], header=None)\n",
    "users.columns = ['USER_ID', 'TITLE']\n",
    "\n",
    "items = pd.read_csv('./beverages.csv', sep=',', usecols=[0,1], header=None)\n",
    "items.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "user_id, item_id, _ = data.sample().values[0]\n",
    "\n",
    "user_title = users.loc[users['USER_ID'] == item_id].values[0][-1]\n",
    "item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "\n",
    "print(\"User:               {}\".format(user_title))\n",
    "print(\"Previous Purchase:  {}\".format(item_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call GetRecommendations\n",
    "\n",
    "The last thing we do is actually make a Personalize recommendations inference call - as you can see from the Code cell this is literally a single line of code with a userID and itemID as input variables (although, strictly speaking, you only need the userID for the datasets that we have).\n",
    "\n",
    "The inference call returns a list of up to 25 itemIDs from the training set - we take that and look up the corresponding beverage name from the *beverages.csv* file and display them; this is far more useful than just a list of ID values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for SUV, New: [\n",
      "  \"Coffee, Bottled\",\n",
      "  \"Coca-Cola/Pepsi\",\n",
      "  \"DrPepper/MrPibb\",\n",
      "  \"Energy Drink\",\n",
      "  \"Mountain Dew/Sierra Mist\",\n",
      "  \"Sports Drink\",\n",
      "  \"Fanta\",\n",
      "  \"Tea\",\n",
      "  \"Sprite/7Up\",\n",
      "  \"Juice\",\n",
      "  \"Root Beer\",\n",
      "  \"Coffee, Store\",\n",
      "  \"Water, Bottled\",\n",
      "  \"Milk\"\n",
      "]\n",
      "\n",
      "Suggestion #1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://bp-personalize-lab-2020.s3.amazonaws.com/images/coffeebottle.jpg\" width=\"150\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggestion #2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://bp-personalize-lab-2020.s3.amazonaws.com/images/coke-pepsi.jpg\" width=\"150\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggestion #3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://bp-personalize-lab-2020.s3.amazonaws.com/images/drpepper-mypibb.jpg\" width=\"150\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(user_id)\n",
    ")\n",
    "\n",
    "item_list = get_recommendations_response['itemList']\n",
    "title_list = [items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1] for item in item_list]\n",
    "\n",
    "print(\"Recommendations for {}: {}\".format(user_title, json.dumps(title_list, indent=2)))\n",
    "\n",
    "%store -r images\n",
    "\n",
    "print(\"\\nSuggestion #1:\")\n",
    "display(Image(url=images[title_list[0]][\"url\"], width=150, height=150))\n",
    "print(\"\\nSuggestion #2:\")\n",
    "display(Image(url=images[title_list[1]][\"url\"], width=150, height=150))\n",
    "print(\"\\nSuggestion #3:\")\n",
    "display(Image(url=images[title_list[2]][\"url\"], width=150, height=150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
